{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamiltonLROliveira/ciencia_dados_puc_rj/blob/main/mvp_finetuning_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MVP da disciplina Machine Learning & Analytics\n",
        "# Hamilton Luiz Rodrigues de Oliveira"
      ],
      "metadata": {
        "id": "Qgx08N0y7YLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descrição do Problema"
      ],
      "metadata": {
        "id": "QsKyZcjzNdMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O projeto surgiu da real necessidade de ter de extrair informações em documentos sigilosos descritos em texto livre para realização das atividades de um órgão público federal. Entre as informações passíveis de extração estão: pessoas jurídicas, pessoas físicas, país, estado, cidade, além de informações inerentes ao texto, como assunto de que trata e elaboração de um resumo.\n"
      ],
      "metadata": {
        "id": "DskevFMT79-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solução Proposta"
      ],
      "metadata": {
        "id": "IpZ-pmvQRyrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A solução proposta consiste na utilização de um modelo de inteligência artificial, Large Language Model(LLM), para a extração das informações desejadas.\n",
        "\n",
        "Considerando que as informações devem ser extraídas de documentos sigilosos foi descartada a possibilidade de utilização de solução de llm fornecida por terceiros, como ChatGPT, por exemplo, uma vez que seria necessária a realização de upload no ambiente dos respectivos fornecedores.\n",
        "\n",
        "Propos-se então a realização de fine tuning numa LLM, pré-treinada, para a extração das informações. Esta tem sido uma prática recorrente na indústria e na academia, qual seja, a adaptação de llms menores para realizar atividades específicas. Tal abordagem torna possível que organizações que não disponham de vultosos recursos financeiros e computacionais possam treinar seus próprios modelos, reduzindo, assim, sua dependência tecnológica dos grandes fornecedores. A título de comparação, a llm utilizada neste projeto possui 7 bilhões de parâmetros, ao passo que o ChatGPT possui mais de 175 bilhões de parâmetros.\n"
      ],
      "metadata": {
        "id": "DHfipFbrRUWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geração do Dataset"
      ],
      "metadata": {
        "id": "7TcE6unxgndY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em virutde da ausência de dados rotulados para a realização do treinamento, foi utilizado o ChatGPT para a geração do dataset que será submetido à llm. Os textos são provenientes de um banco de dados de notícias de diversos veículos de comunicação, em lingua portuguesa, alimentado a partir de um processo diário de webscraping, o qual está fora do escopo deste projeto. Considerando que as notícias são de domínio público não há, portanto, restrições para utilização do ChatGPT para a tarefa de geração do dataset. Desta forma, utilizaremos os dados extraídos pelo ChatGPT como refência para a avaliação da qualidade da extração da llm.\n"
      ],
      "metadata": {
        "id": "64WI5QwThKFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As informações extraídas dos textos de notícias, por meio do ChatGPT, foram:\n",
        "\n",
        "  1. Resumo: Um breve resumo do texto;\n",
        "  2. Transacao_comercial (S/N): O texto trata de transação comercial?\n",
        "  3. Atividade_ilegal (S/N): O texto trata de atididade ilegal?\n",
        "  4. Mensagem_positiva (S/N): A mensagem do texto é positiva ou negativa?\n",
        "  5. Link_publicacao: Apresenta o link de publicação do veículo de comunicação;\n",
        "  6. Paises: A lista de países presentes no texto;\n",
        "  7. Estados: A lista de estados presentes no texto;\n",
        "  8. Cidades: A lista de cidades presentes no texto;\n",
        "  9. Pessoas juridicas:\n",
        "\n",
        "    9.1 A lista de pessoas juridicas presentes no texto;\n",
        "\n",
        "    9.2 Um resumo do que o texto descreve de cada pessoa jurídica;\n",
        "\n",
        "    9.3 O tipo de cada pessoa jurídica, empresa, órgão público, etc\n",
        "    \n",
        "  10. Pessoas físicas:\n",
        "\n",
        "    10.1 A lista de pessoas físicas presentes no texto;\n",
        "    \n",
        "    10.1 Um resumo do que o texto descreve de cada pessoa física"
      ],
      "metadata": {
        "id": "kbmg5DS_NcBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conforme descrito a seguir, foram desenvolvidos três prompts para a obtenção dos dados:\\\n",
        "  1. pessoas jurídicas;\\\n",
        "  2. pessoas físicas; e\\\n",
        "  3. Metadados do texto.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lxct_o2rWueJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_pessoa_juridica = f\"\"\"\n",
        "\n",
        "    Pessoa jurídica é uma construção legal que permite a grupos de pessoas, sejam elas físicas ou outras entidades, atuar como uma única entidade, com direitos e obrigações próprias.\n",
        "    Este conceito é fundamental no mundo dos negócios e do direito, pois fornece um meio para que organizações funcionem de maneira eficaz e sejam responsabilizadas.\n",
        "    As características chave de uma pessoa jurídica incluem: Criação Legal e Reconhecimento: Uma pessoa jurídica é criada através de processos legais e\n",
        "    reconhecida pelas leis de um país ou jurisdição. Este processo varia dependendo da forma e do propósito da organização, podendo incluir a inscrição em um registro oficial,\n",
        "    a emissão de um estatuto social, entre outros.\n",
        "\n",
        "    Capacidade para Exercer Direitos e Obrigações: Uma vez criada, a pessoa jurídica tem a capacidade de exercer direitos e assumir obrigações.\n",
        "    Isso significa que ela pode possuir propriedades, contratar funcionários, entrar em contratos, tomar empréstimos, e realizar outras atividades comerciais como qualquer indivíduo.\n",
        "\n",
        "    Responsabilidade Legal Separada: Uma característica importante da pessoa jurídica é a separação entre a entidade e seus membros ou acionistas.\n",
        "    Isso significa que os membros da pessoa jurídica geralmente não são pessoalmente responsáveis pelas dívidas ou obrigações legais da entidade.\n",
        "    Esta separação protege os ativos pessoais dos indivíduos em caso de problemas legais ou financeiros da entidade.\n",
        "    Tipos de Pessoas Jurídicas: Existem vários tipos de pessoas jurídicas, cada uma com suas próprias regras e regulamentos. Estes incluem corporações, associações,\n",
        "    fundações, entidades governamentais e organizações sem fins lucrativos. Cada tipo tem características específicas quanto à estrutura, tributação e governança.\n",
        "\n",
        "    Tributação: As pessoas jurídicas são geralmente sujeitas a um regime tributário próprio, que pode diferir significativamente da tributação de pessoas físicas.\n",
        "    As regras tributárias variam de acordo com a jurisdição e o tipo da pessoa jurídica.\n",
        "\n",
        "    Perpetuidade: Ao contrário das pessoas físicas, uma pessoa jurídica pode continuar a existir independentemente das mudanças em seus membros ou proprietários.\n",
        "    Isso oferece estabilidade e continuidade nos negócios e nas operações da entidade.\n",
        "\n",
        "    Em resumo, a pessoa jurídica é uma ferramenta legal essencial para a organização e operação de uma ampla gama de atividades comerciais e não comerciais,\n",
        "    fornecendo um meio para que as entidades atuem e sejam reconhecidas de forma coletiva perante a lei.\n",
        "\n",
        "    Em relação aos tipos de pessoas jurídicas, qualifique as como empresas, órgãos públicos, partidos políticos sindicatos/associações, ou outras\n",
        "\n",
        "    Com base nos conceitos acima, para o texto a seguir, responda a seguinte pergunta:\n",
        "    Quais são os nomes de pessoas jurídicas mencionadas neste texto de notícia, incluindo nomes de países, organismos internacionais,\n",
        "    órgãos publicos, empresas comerciais, ministérios, sindicatos ou quaisquer outras entidades.\n",
        "    Em hipóstese nenhuma capture nomes de pessoas físicas.\n",
        "\n",
        "    As informações a serem extraídas são o nome da pessoa jurídica e um resumo do que o texto aborda sobre a pessoa jurídica.\n",
        "    A saída deve ser formatada em JSON com os seguintes campos: Nome, tipo_pessoa, Resumo_referente_a_pessoa.\n",
        "\n",
        "\n",
        "    ```text```\n",
        "    \"\"\"\n",
        "\n",
        "prompt_pessoa_fisica = f\"\"\"\n",
        "\n",
        "    Pessoa física pode ser conceituada como um ser humano individual. No contexto legal e contábil,\n",
        "    o termo \"pessoa física\" é usado para se referir a um indivíduo em contraste a uma \"pessoa jurídica\",\n",
        "    que é uma entidade legal, como uma empresa, organização sem fins lucrativos ou governo.\n",
        "    Pessoas físicas têm identidades únicas, direitos e responsabilidades legais. Elas podem possuir bens, assinar contratos,\n",
        "    realizar transações financeiras, pagar impostos e participar de atividades sociais e econômicas. No sistema legal,\n",
        "    as pessoas físicas têm direitos e proteções fundamentais, como liberdade de expressão,\n",
        "    direito à propriedade e acesso ao devido processo legal em caso de litígios.\n",
        "\n",
        "    Considerando que pessoas físicas devem ser identificadas individualmente, não descreva na mesma linha mais de uma pessoa física.\n",
        "    Sendo assim, utilize uma linha de saída para cada pessoa física. Por exemplo, na frase: \"João e Maria foram ao supermercado\", aparecem duas pessoas físicas,\n",
        "    João e Maria, deve-se utilizar uma linha para especificar as informações de João e outra linha para especificar as informações de Maria\n",
        "    ou seja, devem ser descritas individualmente uma linha para cada pessoa física. Reforçando, apenas uma pessoa deve ser descrita por linha.\n",
        "    Outro exemplo, na frase 'Luíza Benamor e João Leme, analistas da consultoria Tendências, que mantiveram a estimativa de alta de 5,1% para o IPCA no ano'\n",
        "    Luíza Benamor é uma pessoa física e João Leme é outra pessoa física e devem ser descritos separadamente.\n",
        "    Retire todos os caracters como acento, til e cedilha, substituindo pelas mesm\n",
        "\n",
        "    Com base nos conceitos acima, para o texto a seguir, responda a seguinte pergunta:\n",
        "    Quais são os nomes de pessoas físicas mencionadas neste texto de notícia, excluindo nomes de países, organismos internacionais,\n",
        "    órgãos publicos, pessoas juridicas, ministérios, sindicatos ou quaisquer outras entidades.\n",
        "    As informações a serem extraídas são o nome da pessoa física e um resumo do que o texto aborda sobre a pessoa física.\n",
        "\n",
        "    A saída deve ser formatada em JSON com os seguintes campos: Nome, Resumo_referente_a_pessoa.\n",
        "\n",
        "    ```text```\n",
        "    \"\"\"\n",
        "\n",
        "prompt_resumo = f\"\"\"\n",
        "\n",
        "    Uma transação comercial é um processo em que ocorre a compra e venda de bens ou serviços entre duas partes, \\\n",
        "    geralmente envolvendo a troca de dinheiro. Essas transações podem ocorrer entre empresas (B2B), entre empresas e consumidores (B2C) \\\n",
        "    ou entre consumidores (C2C). Elas são fundamentais para a economia, pois permitem a transferência de recursos e a geração de receita. \\\n",
        "    As transações comerciais podem ser realizadas de forma presencial, por telefone, online ou por meio de plataformas digitais. \\\n",
        "    Elas envolvem negociação, acordo de preço, entrega do produto ou serviço e pagamento. Além disso, as transações comerciais estão sujeitas a regulamentações legais\n",
        "    e contratuais para garantir a proteção dos direitos e interesses das partes envolvidas.\n",
        "\n",
        "    Uma atividade ilegal refere-se a qualquer ação, prática ou comportamento que vá contra as leis, \\\n",
        "    regulamentos e normas estabelecidas pelo ordenamento jurídico brasileiro. Essas atividades são consideradas ilegais \\\n",
        "    porque violam as leis do país e podem resultar em consequências legais, incluindo sanções penais, civis e administrativas, \\\n",
        "    dependendo da natureza da infração e das leis aplicáveis.\n",
        "    As atividades ilegais no Brasil podem abranger uma ampla gama de comportamentos, como por exemplo: \\\n",
        "    Roubo, Furto, Extorsão, Dano, Apropriação indébita, Estelionato, Receptação, Corrupção, Concussão, \\\n",
        "    Peculato, Prevaricação, Lavagem de dinheiro, Advocacia administrativa, Sonegação fiscal, Crime contra o sistema financeiro, Fraudes em licitações\n",
        "    Por favor, não transcreva esse texto no resumo mas sim como instruções para elaboração do resumo do texto.\n",
        "\n",
        "    Com base nos conceitos acima, considere que você é um assistente e deve realizar as tarefas a seguir:\n",
        "\n",
        "    1 - Fazer um resumo do texto;\n",
        "    2 - Identificar se o texto trata de transação comercial, com respostas Sim ou Não;\n",
        "    3 - Informe se o texto trata de atividade ilegal, com respostas Sim ou Não;\n",
        "    4 - Informe se o texto trás uma mensagem positiva ou negativa, com respostas Sim ou Nâo;\n",
        "    5 - Extraia o link de publicação que se encontra no texto.\n",
        "\n",
        "    Os dados de saída devem ser no formato JSON, com os seguintes campos: resumo, transacao_comercial, atividade_ilegal, mensagem_positiva, link_publicacao\n",
        "    substiuindo os valores True para Sim e False para Não.\n",
        "\n",
        "    ```text```\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "Ly1UoRd14VSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como resultado da aplicação dos prompts ao ChatGPT, foram gerados três arquivos JSON, um para cada chamada à API. No total, foram submetidas 5.196 notícias, que resultou nos resultados descritos a seguir:"
      ],
      "metadata": {
        "id": "LK0m0q_qYdkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As notícias selecionadas são lidas do arquivo noticias.txt e carregadas no dataframe df_noticias\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "noticias_dir = '/content/drive/MyDrive/data/noticias_processadas/'\n",
        "df_noticias = pd.read_csv(noticias_dir + 'noticias.txt')\n",
        "df_noticias['DatPublicacaoMateria'] = pd.to_datetime(df_noticias['DatPublicacaoMateria'])\n",
        "df_noticias"
      ],
      "metadata": {
        "id": "J_sQiTUIdpiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Neste gráfico são apresentadas as notícias distribuídas por ano. Pode-se observar que as notícias estão ordenadas por ano, tendo notícias de 2003 a 2024.\n",
        "\n",
        "df = df_noticias[['DatPublicacaoMateria']]\n",
        "df_eixos = df.groupby(df['DatPublicacaoMateria'].dt.to_period('Y')).count()\n",
        "df_eixos.columns = [\"Qtde\"]\n",
        "df_eixos.index=df_eixos.index.to_series().astype(str)\n",
        "anos = df_eixos.index.to_list()\n",
        "df_eixos = df_eixos.reset_index(drop=True)\n",
        "df_eixos.insert(0, \"Ano\", anos)\n",
        "df_eixos_ordem_qtde = df_eixos.sort_values([\"Ano\"] , ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (15,6))\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.rc(\"axes\", facecolor=\"#D3D3D3\", grid = True)\n",
        "plt.rc(\"grid\", color=\"#FFFFFF\")\n",
        "plt.tight_layout()\n",
        "plt.bar(df_eixos_ordem_qtde[\"Ano\"], df_eixos_ordem_qtde[\"Qtde\"], color='#6688AA')\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.yscale('log')\n",
        "plt.title(\"Quantidade de noticias por ano\")\n",
        "plt.xlabel(\"Ano\")\n",
        "plt.ylabel(\"Quantidade de noticias\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d-nLS2XRqbZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Neste gráfico são apresentadas as notícias distribuídas por ano. Pode-se observar que as notícias estão ordenadas por quantidade de notícias por ano, sendo que 2024 é o ano com maior\n",
        "#número de notícias, 4.394, e os anos de 2007 e 2012, os anos com menor número de notícias, 01, cada.\n",
        "\n",
        "df = df_noticias[['DatPublicacaoMateria']]\n",
        "df_eixos = df.groupby(df['DatPublicacaoMateria'].dt.to_period('Y')).count()\n",
        "df_eixos.columns = [\"Qtde\"]\n",
        "df_eixos.index=df_eixos.index.to_series().astype(str)\n",
        "anos = df_eixos.index.to_list()\n",
        "df_eixos = df_eixos.reset_index(drop=True)\n",
        "df_eixos.insert(0, \"Ano\", anos)\n",
        "df_eixos_ordem_qtde = df_eixos.sort_values([\"Qtde\"] , ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (15,6))\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.rc(\"axes\", facecolor=\"#D3D3D3\", grid = True)\n",
        "plt.rc(\"grid\", color=\"#FFFFFF\")\n",
        "plt.tight_layout()\n",
        "plt.bar(df_eixos_ordem_qtde[\"Ano\"], df_eixos_ordem_qtde[\"Qtde\"], color='#6688AA')\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.yscale('log')\n",
        "plt.title(\"Quantidade de noticias por ano\")\n",
        "plt.xlabel(\"Ano\")\n",
        "plt.ylabel(\"Quantidade de noticias\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YI3jcwWFd5GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando a complexidade do projeto e o extensão do processo necessário para extração e tratamento de todas as informações desejadas, no contexto deste MVP, abordaremos a extração apenas da pessoas jurídicas citadas no texto, bem como seus respectivos tipos. Deste modo, abordaremos a seguir apenas às informações referentes à captura de pessoas jurídicas, ignorando, portanto, aos demais dados extraídos dos textos."
      ],
      "metadata": {
        "id": "XdMNU__3l1-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Carrega o arquivo com todos os tipos capturados pelo ChatGPT\n",
        "import pandas as pd\n",
        "path = '/content/drive/MyDrive/data/noticias_processadas/'\n",
        "df_tipos_pj = pd.read_csv(path + 'tipo_pj.csv')\n",
        "df_tipos_pj"
      ],
      "metadata": {
        "id": "UB867ZyzTSRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apresenta o número total diferentes tipos capturados pelo ChatGPT\n",
        "\n",
        "len(df_tipos_pj['tipo_pessoa'].unique())"
      ],
      "metadata": {
        "id": "Hdbyv92_TiCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando a grande quantidade de diferentes tipos, 763, com um número elevado de inconsistências nos tipos capturados pelo ChaGPT foi necessária a realização de um tratamento de seleção e agregação nos dados capturados de forma a tornar possível a geração do dataset para treinar a llm.\n",
        "\n",
        "Desta forma, foram identificados e tratados apenas sete tipos iniciais, que são: empresa, órgão público, partido político, associação/entidade de classe e fundação.\n",
        "\n",
        "Considerando, ainda, a demora no processamento do fine tuning realizado no cluster, dada a grande quantidade de dados, optou-se por mais um refinamento, que consistiu na obtenção das 200 notícias de cada tipo, com maior número de ocorrência de captura de pessoas jurídicas, tendo se obtido um total de 935 notícias, conforme descrito no gráfico à seguir. Esse é o dataset que está sendo utilizado neste MVP."
      ],
      "metadata": {
        "id": "BcYKc61aYE8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Função utilizada para carga do dataset.\n",
        "\n",
        "import json\n",
        "def ler_arquivo(path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for ff in files:\n",
        "            if ff.lower().endswith(\".json\"):\n",
        "                yield os.path.join(root, ff)"
      ],
      "metadata": {
        "id": "BQIb-itOW2QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variável que define a quantidade de exemplos que serão utilizados para treinamento. O dataset tem um número máximo de 935exemplos, o que demanda aproximadamente 11m, rodando na placa GPU A100.\n",
        "#Ela é utilizada na chamada da função ler_arquivo() que carrega o dataset da pasta: '/content/drive/MyDrive/data/ds_llm2gov_input'\n",
        "#Para poder testar todo o ciclo do fine tuning pode-se definir um valor baixo para teste, como 10 exemplos, o que permite a execução de todo o processo em poucos minutos.\n",
        "\n",
        "num_exemplos = 935"
      ],
      "metadata": {
        "id": "Hzav42AjaKoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chamada da função que lê os arquivos do diretório e carrega na memória utilizando a lib datasets, do Hugginface\n",
        "#Embaralhar a amostra de treino e guardar na pasta\n",
        "\n",
        "import os\n",
        "ds_path = '/content/drive/MyDrive/data/ds_llm2gov_input/'\n",
        "conteudos = []\n",
        "for arquivo in list(ler_arquivo(ds_path))[:num_exemplos]:\n",
        "    with open(arquivo, 'r') as f:\n",
        "        conteudos.append(json.load(f))\n",
        "\n",
        "df_ner = pd.DataFrame(conteudos)"
      ],
      "metadata": {
        "id": "X695jHvfZr_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ner"
      ],
      "metadata": {
        "id": "_Va3jh1DfDgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percebeu-se, no entanto, que utilizar todos esses dados para a realização do fine tuning na llm não seria uma tarefa trivial, motivo pelo qual decidiu-se pela realização primeiramente da captura de pessoas jurídicas, para então prosseguir com a extração dos demais dados capturados na interação com o ChatGPT. Diante dessa limitação, este MVP trata apenas da caputura de pessoas jurídicas e seus respectivos tipos."
      ],
      "metadata": {
        "id": "XACW3fn7US4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adiante, é apresentado um detalhamento dos dados que será utilizados no fine tuning.\n",
        "\n",
        "print(df_ner['texto'][0]) #Texto da notícia na primeira posição do dataframe"
      ],
      "metadata": {
        "id": "0JU6B2lkfNf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As pessoas jurídicas capturadas nessa notícia foram: TRIBUNAL DE CONTAS DA UNIAO, MINISTERIO DA GESTAO E INOVACAO EM SERVICOS PUBLICOS, BANCO DO BRASIL, CAIXA ECONOMICA FEDERAL, CORREIOS,\n",
        "#CODEVASF e PETROBRAS, conforme se observa a seguir.\n",
        "#Observa-se que o Banco Nacional de Desenvolvimento Econômico e Social (BNDES), embora conste do texto, não foi capturado pelo ChatGPT, configurando numa inconsistência.\n",
        "\n",
        "df_ner['pessoas_juridicas'][0]"
      ],
      "metadata": {
        "id": "S-UgOSl_feSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construção e apresentação do gráfico com os cinco tipos de pessoas jurídicas selecionados para a realização do fine tuning.\n",
        "\n",
        "tipo_pessoa_ls = []\n",
        "for index, row in df_ner.iterrows():\n",
        "  #print(row['pessoas_juridicas'])\n",
        "  for item in row['pessoas_juridicas']:\n",
        "    if item['TP_PESSOA'] not in ['PAIS', 'CIDADE','ESTADO']:\n",
        "      #print(item['TP_PESSOA'])\n",
        "      tipo_pessoa_ls.append(item['TP_PESSOA'])\n",
        "\n",
        "df_tp = pd.DataFrame({'Tipo':tipo_pessoa_ls})\n",
        "df_tp\n",
        "\n",
        "df_eixos =  df_tp.groupby(['Tipo']).size().to_frame()\n",
        "df_eixos.columns = [\"Qtde\"]\n",
        "tipos = df_eixos.index.to_list()\n",
        "df_eixos = df_eixos.reset_index(drop=True)\n",
        "df_eixos.insert(0, \"TP_Pessoas\", tipos)\n",
        "df_eixos_ordem_qtde = df_eixos.sort_values([\"Qtde\"] , ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,4))\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.rc(\"axes\", facecolor=\"#D3D3D3\", grid = True)\n",
        "plt.rc(\"grid\", color=\"#FFFFFF\")\n",
        "plt.tight_layout()\n",
        "plt.barh(df_eixos_ordem_qtde[\"TP_Pessoas\"], df_eixos_ordem_qtde[\"Qtde\"], color='#6688AA')\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.xscale('log')\n",
        "plt.title(\"Tipos de pessoas jurídicas\")\n",
        "plt.ylabel(\"Tipos\")\n",
        "plt.xlabel(\"Quantidade\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5gaX32qWkcaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforma o dataframe utilizando a biblioteca Datasets, do Hugginface.\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "8zZYVAC1eX3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "ds_ner = datasets.Dataset.from_pandas(df_ner)"
      ],
      "metadata": {
        "id": "SE2DqBuU-2R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_ner"
      ],
      "metadata": {
        "id": "l8g0Fw4CfJVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# bibliotecas\n",
        "########################################\n",
        "!pip install -q accelerate transformers evaluate peft trl sentencepiece bitsandbytes flash-attn torch"
      ],
      "metadata": {
        "id": "nVibdf9t4TAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKc3G6gu3Rcl"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# imports\n",
        "########################################\n",
        "import gc\n",
        "import json\n",
        "import pdb\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import traceback\n",
        "import warnings\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "from datasets import load_dataset\n",
        "\n",
        "import evaluate\n",
        "import flash_attn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import tqdm\n",
        "import transformers\n",
        "import trl\n",
        "\n",
        "\n",
        "from peft import get_peft_model\n",
        "from peft import LoraConfig\n",
        "from peft import PeftConfig\n",
        "from peft import PeftModelForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import default_data_collator\n",
        "from transformers import GenerationConfig\n",
        "from transformers import pipeline\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import StoppingCriteria\n",
        "from transformers import StoppingCriteriaList\n",
        "from trl import SFTTrainer\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKoh_biw4i-E"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# config\n",
        "########################################\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "transformers.set_seed(SEED)\n",
        "\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('datasets', datasets.__version__)\n",
        "print('evaluate', evaluate.__version__)\n",
        "print('bitsandbytes', bitsandbytes.__version__)\n",
        "print('flash_attn', flash_attn.__version__)\n",
        "print('accelerate', accelerate.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK4uKAN9uTp2"
      },
      "outputs": [],
      "source": [
        "#Variável utilizada para definir o valor do parâmetro  max_seq_length, da classe SFTTrainer, que será explicada mais à frente. Este parâmetro determina o tamanho da sentença que será utilizada no treinamento.\n",
        "#A limitação desse parâmetro afeta bastante o resultado final do treinamento, a título de exemplo, executando esse notebook num cluster linux é possível definir o parâmetro em 8000, o que possibilita\n",
        "#um melhor resultado do modelo após a realização do fine-tuning. Em que pese a limitação do tamanho em virtude da pouca memória disponível, o treinamento do modelo com um valor máximo de 2000\n",
        "#já permite obter uma melhora do modelo na caputura das entidades, conforme será demonstrado ao longo da aplicação.\n",
        "#Utilizada em conjunto com FINETUNING_PATH define onde o modelo tunado será armazenado\n",
        "\n",
        "#Definindo 100 tokens consome no máximo 7.8GB - Executa em 20 minutos numa Placa T4 que possui GPU com 15GB. O teste de inferência já não funciona numa T4 pois consome mais que 15GB de RAM do sistema\n",
        "#Definindo 256 tokens consome no máximo 10GB - Executa em 20 minutos numa Placa T4 que possui GPU com 15GB. O teste de inferência já não funciona numa T4 pois consome mais que 15GB de RAM do sistema\n",
        "#Definindo 2048 tokens-consome no máximo 35GB  - Executa em 10 minutos numa Placa A100 que possui GPU com 40GB - Inferência funciona sem problemas, consome alguns poucos minutos\n",
        "\n",
        "SENTENCE_MAX_LENGTH = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Foi aplicado um fine tuning no odelo original fernandosola/bluearara-7B?\n",
        "#Variável que define o modelo base que será utilizado para o fine tuning disponível na plataforma Hugginface. A biblioteca dataset, do Hugginface procura na plataforma de acordo com o nome para realizar\n",
        "#o upload, mas também funciona com cache. Ou seja, se o modelo já estiver downloaded, ela utiliza o modelo que já está salvo, ao invés de realizar o download. Essa é a opção que estamos utilizando e a variável\n",
        "#que define o diretório do modelo downloaded é HF_CACHE_DIR, que será descrita adiante.\n",
        "\n",
        "BASE_MODEL = 'fernandosola/bluearara-7B-instruct'\n",
        "#BASE_MODEL = '/content/drive/MyDrive/data/Llm2GovBR-Mistral-v0.1-7B-Instruct-sharded-500M-plus-pad'"
      ],
      "metadata": {
        "id": "UeXNqLY8wkmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define o diretório onde será gravado o modelo resultado do fine-tuning. Considerando a possibilidade de ocorrência de problemas no ambiente no momento da execução pelos professores estou deixando um modelo tunado\n",
        "#no diretório /content/drive/MyDrive/data/model/mistral2govbr_ner' gerado com o parâmetro max_seq_lengthe fixado em 2000.\n",
        "#O modelo novo que será gerado na execução realizada pelos professor vai ser gravado no diretório com o sufixo '_novo'\n",
        "\n",
        "FINETUNING_PATH = f'/content/drive/MyDrive/data/model/mistral2govbr_ner_' + str(SENTENCE_MAX_LENGTH) + '/'"
      ],
      "metadata": {
        "id": "EeaXMIRNz9__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variável que define a pasta cache do Hugginface\n",
        "\n",
        "HF_CACHE_DIR = '/content/drive/MyDrive/data/hfcache/models/'"
      ],
      "metadata": {
        "id": "A1TYfsRGysmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU6zZig7eIto"
      },
      "outputs": [],
      "source": [
        "#Chamada da função que lê os arquivos do diretório e carrega na memória utilizando a lib datasets, do Hugginface\n",
        "#Embaralhar a amostra de treino e guardar na pasta\n",
        "\n",
        "conteudos = []\n",
        "for arquivo in list(ler_arquivo(ds_path))[:num_exemplos]:\n",
        "    with open(arquivo, 'r') as f:\n",
        "        conteudos.append(json.load(f))\n",
        "\n",
        "ds_ner = pd.DataFrame(conteudos)\n",
        "ds_ner = datasets.Dataset.from_pandas(ds_ner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix7YQQ5rnP5k"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir desse ponto iniciam-se efetivamente os procedimentos para a realização do fine tuning do modelo. O modelo que será utilizado como base é o modelo bluearara-7B-instruct, disponível também na plataforma Hugginface. Vale ressaltar que o bluearara-7B-instruct é resultado de um fine tuning no modelo bluearara-7B, com um dataset com aproximadamente 34 mil instruções, de modo que o modelo reusultante tivesse mais facilidade para realizar tarefas atráves de instruções."
      ],
      "metadata": {
        "id": "_sitRu0rB0Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A classe AutoTokenizer é uma classe genérica que retorna o tokenizador do modelo pré-treinado 'fernandosola/bluearara-7B-instruct' passado como parâmetro, em BASE_MODEL\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "\n",
        "    #modelo base pré-treinado, fernandosola/bluearara-7B-instruct\n",
        "    BASE_MODEL,\n",
        "\n",
        "    #Indica se deve ser carregada a versão rápida do tokenizador(True) ou se deve ser utilizada a versão em Python(False)\n",
        "    use_fast=False,\n",
        "\n",
        "    #token que permite acessar o modelo\n",
        "    token='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'\n",
        ")"
      ],
      "metadata": {
        "id": "5V2aGyjpBahO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWe2tpKlpYlJ"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpD5ZwegpURT"
      },
      "outputs": [],
      "source": [
        "#Chamada ao tokenizador instanciado\n",
        "\n",
        "tokenizer.tokenize(\"teste\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adição do token especial\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "tokenizer.padding_side = 'right'"
      ],
      "metadata": {
        "id": "IuuycwDqKe4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7Er__OnuQhVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carga do modelo base fernandosola/bluearara-7B-instruct que será utilizado no fine tuning\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(  #Classe da biblioteca Transformers que instancia o modelo descrito em BASE_MODEL\n",
        "\n",
        "  #modelo base pré-treinado, fernandosola/bluearara-7B-instruct\n",
        "  BASE_MODEL,\n",
        "\n",
        "  #Quantização é uma técnica que permite a redução dos custos computacionais e de memória por meio da redução da precisão dos pesos.\n",
        "  quantization_config=BitsAndBytesConfig(\n",
        "\n",
        "        #Configura a quantização para 4 bits\n",
        "        load_in_4bit=True,\n",
        "\n",
        "        #Habilita a quantização aninhada\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "\n",
        "        #Define o tipo de dado da quantização para nf4\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    ),\n",
        "\n",
        "    #Possibilita o treinamento distribuído do modelo\n",
        "    device_map=\"auto\",\n",
        "\n",
        "    #Permite ou não a carga de modelos customizados\n",
        "    trust_remote_code=False,\n",
        "\n",
        "    #Diretório utilizado como cache dos arquivos de vocabulário do tokenizador\n",
        "    cache_dir=HF_CACHE_DIR,\n",
        "\n",
        "    #Força ou não o download dos arquivos de vocabulário sobrescrevendo os arquivos existentes\n",
        "    force_download=False,\n",
        "\n",
        "    #Deleta ou não arquivos incompletos\n",
        "    resume_download=True,\n",
        "\n",
        "    #token para acesso ao modelo na plataforma Hugginface\n",
        "    token='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',\n",
        ")\n"
      ],
      "metadata": {
        "id": "Pn6gubpXKhJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variável que aponta para o modelo base instanciado\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "qHh72DNmd92C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j-ZTtc04FEI"
      },
      "source": [
        "# Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qevK-OSVx1x4"
      },
      "outputs": [],
      "source": [
        "#Adiciona um elemento ao vocabulário do tokenizador por conta do token pad\n",
        "\n",
        "model.resize_token_embeddings(model.config.vocab_size + 1)\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "generation_args = {\n",
        "    'pad_token_id': tokenizer.pad_token_id,\n",
        "    'eos_token_id': tokenizer.eos_token_id,\n",
        "    'bos_token_id': tokenizer.bos_token_id,\n",
        "    'do_sample': False,\n",
        "    'num_beams': 1,\n",
        "    #'temperature': 0,\n",
        "}\n"
      ],
      "metadata": {
        "id": "i5tQsf4FnRMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH6I0uZSl6qX"
      },
      "outputs": [],
      "source": [
        "#Pipelines é um método para instanciar modelos para realizar inferência. A realização do teste de inferência no modelo base não tem utilidade para a realização do fine tuning.\n",
        "#Estou realizando esse procedimento com o intuito de pode comparar a qualidade da resposta do modelo base com a qualidade da resposta do modelo após o fine tuning.\n",
        "\n",
        "pipe = pipeline(task='conversational', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ8M6vg4lcPm"
      },
      "outputs": [],
      "source": [
        "#Teste de inferência no modelo base\n",
        "pergunta = '''\\\n",
        "Identifique e extraia as informações de nomes de pessoas jurídicas mencionadas no texto, incluindo nomes de órgãos publicos, \\\n",
        "empresas, partidos políticos, associações, sindicatos ou quaisquer outras entidades. Não extraia nomes de pessoas físicas.\\\n",
        "O resultado deve ser apresentado em forma de lista de pessas jurídicas conforme o exemplo abaixo:\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Petrobrás é uma empresa. José da Silva é uma pessoa física.\n",
        "\n",
        "Pessoas Jurídicas: Petrobrás|Empresa\n",
        "\n",
        "Faça o mesmo para o seguinte texto:\n",
        "\n",
        "No fervilhante cenário político e econômico do Brasil, onde as decisões do Supremo Tribunal Federal e as políticas do Ministério da Economia \\\n",
        "têm repercussões que ecoam por todo o país, é comum observar figuras proeminentes como Maria Silva, CEO da renomada empresa de tecnologia \\\n",
        "Innovatech, buscando navegar entre os interesses corporativos e as demandas sociais. Enquanto isso, José Santos, um respeitado advogado, \\\n",
        "se destaca por sua atuação na defesa dos direitos individuais em meio às controvérsias jurídicas que frequentemente alcançam os tribunais \\\n",
        "superiores. Nas salas de reuniões, executivos como Ana Oliveira e Carlos Almeida da Vale Energia negociam estratégias de expansão \\\n",
        "e investimentos diante das flutuações do mercado global, enquanto o Ministério da Saúde, liderado pelo Ministro Pedro Lima, enfrenta desafios \\\n",
        "sem precedentes na gestão da saúde pública, especialmente em tempos de crise como a pandemia de COVID-19.\n",
        "'''\n",
        "\n",
        "output = pipe([\n",
        "    {'role': 'user', 'content': pergunta},\n",
        "], **generation_args)\n",
        "\n",
        "print(output.messages[-1]['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conforme descrito acima, a resposta ao teste de inferência do modelo base foi:\n",
        "\n",
        "Pessoas Jurídicas:\n",
        "\n",
        "Innovatech|Empresa\\\n",
        "Advogado: José Santos\\\n",
        "Executivos: Ana Oliveira, Carlos Almeida\\\n",
        "Ministério da Saúde: Pedro Lima\n",
        "\n",
        "O modelo acerta para a empresa Innovatech e erra em todas as demais, capturando pessoas físicas e jurídicas de forma incorreta.\n",
        "\n",
        "A resposta correta seria:\n",
        "\n",
        "Pessoas jurídicas:\n",
        "\n",
        "Supremo Tribunal Federal | Órgão Público\\\n",
        "Ministério da Economia | Órgão público\\\n",
        "Innovatech | Empresa\\\n",
        "Vale Energia | Empresa\\\n",
        "Ministério da Saúde | Órgão público\n",
        "\n",
        "\n",
        "Após a realização do fine tuning faremos o mesmo teste de inferência no modelo resultante."
      ],
      "metadata": {
        "id": "IQw9m3kKpAWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLRSo7W3x4ro"
      },
      "outputs": [],
      "source": [
        "#Parameter-Efficient Fine-Tuning (PEFT) permite a adaptação de modelos pré-treinados para aplicações específicas sem que seja necessário ajustar todos os parâmetros do modelo base.\n",
        "#PEFT faz uso da técnica Low-Rank Adaptation of Large Language Models (LoRA)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM-VECmbzCIZ"
      },
      "outputs": [],
      "source": [
        "ta = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir=FINETUNING_PATH,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=10,\n",
        "    bf16=False,\n",
        "    fp16=True,\n",
        "    eval_steps=10,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=10,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=1,\n",
        "    logging_first_step=True,\n",
        "    # load_best_model_at_end=False,\n",
        "    # metric_for_best_model=\"eval_loss\",\n",
        "    seed=SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adiciona os tokens especiais [INST] e [/INST]. Veremos mais adiante que estes tokens serão utilizados para delimitar o ponto na sentença onde se localizam a instrução/pergunta\n",
        "#juntamente com o texto da notícia e o rótulo/resposta do exemplo, que vem a ser as pessoas jurídicas presentes no texto com seus tipos a serem capturados.\n",
        "\n",
        "instruction_template = tokenizer.encode('[INST]', add_special_tokens=False)\n",
        "response_template = tokenizer.encode('[/INST]', add_special_tokens=False)"
      ],
      "metadata": {
        "id": "nkVWGTi6M_7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aqP7NRDzDUy"
      },
      "outputs": [],
      "source": [
        "def to_template(x):\n",
        "    pergunta = f'''\\\n",
        "Identifique e extraia as informações de nomes de pessoas jurídicas mencionadas no texto, incluindo nomes de órgãos publicos, \\\n",
        "empresas, partidos políticos, associações, sindicatos ou quaisquer outras entidades. Não extraia nomes de pessoas físicas.\\\n",
        "O resultado deve ser apresentado em forma de lista de pessas jurídicas conforme o exemplo abaixo:\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Petrobrás é uma empresa. José da Silva é uma pessoa física.\n",
        "\n",
        "Pessoas Jurídicas: Petrobrás|Empresa\n",
        "\n",
        "Faça o mesmo para o seguinte texto:\n",
        "\n",
        "<texto>\n",
        "{x['texto']}\n",
        "</texto>\n",
        "'''\n",
        "\n",
        "    if not x.get('pessoas_juridicas'):\n",
        "        x['pessoas_juridicas'] = []\n",
        "\n",
        "    resposta = f'''\\\n",
        "Pessoas Juridicas: {';'.join(f\"{t['Nome']}|{t['TP_PESSOA']}\" for t in x['pessoas_juridicas'])}\n",
        "'''\n",
        "\n",
        "    messages = [\n",
        "       {\"role\": \"user\", \"content\": pergunta},\n",
        "       {\"role\": \"assistant\", \"content\": resposta},\n",
        "    ]\n",
        "    return {'text': tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "\n",
        "\n",
        "for item in ds_ner:\n",
        "  try:\n",
        "      print(to_template(item))\n",
        "      break\n",
        "  except:\n",
        "      print(item)\n",
        "      to_template(item)\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-4wlplYnL36"
      },
      "outputs": [],
      "source": [
        "ds = ds_ner.map(to_template)\n",
        "ds = ds.train_test_split(.01, seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Z45Lz3nns4"
      },
      "outputs": [],
      "source": [
        "#Com todos os campos  'resumo', 'transacao_comercial', 'atividade_ilegal', ....\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxFQKfzTU3Lc"
      },
      "outputs": [],
      "source": [
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Av2Qy0JJ61H"
      },
      "outputs": [],
      "source": [
        "dc = DataCollatorForCompletionOnlyLM(\n",
        "    response_template=response_template,\n",
        "    instruction_template=instruction_template,\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    me = {'bleu':0}\n",
        "    return me\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=ta,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=ds['train'],\n",
        "    eval_dataset=ds['test'],\n",
        "    dataset_text_field='text',\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=SENTENCE_MAX_LENGTH,\n",
        "    packing=False,\n",
        "    # preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    # data_collator=dc,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(FINETUNING_PATH)\n",
        "\n",
        "# del(tokenizer)\n",
        "# del(model)\n",
        "# del(trainer)\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes de Inferência"
      ],
      "metadata": {
        "id": "6Usmdo7ji9CV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbExl3eL7xKI"
      },
      "source": [
        "##Inferência no modelo gerado com 256 tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variável que define o diretório onde se encontra o modelo já tunado e que será carregado para a realização de testes de inferência. Estou deixando apontando para o modelo que já realizei o texto\n",
        "#com max_seq_lengthe fixado em 2048.\n",
        "\n",
        "#Aqui define apenas o diretório onde o modelo para teste de inferência será carregado, 256 ou 2048\n",
        "SENTENCE_MAX_LENGTH = 100\n",
        "\n",
        "PEFT_MODEL = f'/content/drive/MyDrive/data/model/mistral2govbr_ner_' + str(SENTENCE_MAX_LENGTH) + '/'"
      ],
      "metadata": {
        "id": "lRkDny36vBLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnRAlE9a8g-V"
      },
      "outputs": [],
      "source": [
        " #Teste de inferência no modelo tunado\n",
        "print('Carregando o modelo de ' + PEFT_MODEL)\n",
        "\n",
        " model = AutoModelForCausalLM.from_pretrained(\n",
        "     pretrained_model_name_or_path='fernandosola/bluearara-7B-instruct',\n",
        "     # load_in_4bit=True,\n",
        "     # quantization_config=BitsAndBytesConfig(\n",
        "     #     load_in_4bit=True,\n",
        "     #     bnb_4bit_compute_dtype=torch.float16,\n",
        "     #     bnb_4bit_use_double_quant=True,\n",
        "     #     bnb_4bit_quant_type=\"nf4\",\n",
        "     # ),\n",
        "     # torch_dtype=torch.bfloat16,\n",
        "     cache_dir=HF_CACHE_DIR,\n",
        "     force_download=False,\n",
        "     resume_download=False,\n",
        "     token='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'\n",
        " )\n",
        "\n",
        "\n",
        " tokenizer = AutoTokenizer.from_pretrained(\n",
        "     PEFT_MODEL,\n",
        "     use_fast=False,\n",
        "     cache_dir=HF_CACHE_DIR,\n",
        "     token='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'\n",
        " )\n",
        "\n",
        "\n",
        "peft_model = PeftModelForCausalLM.from_pretrained(\n",
        "    model=model,\n",
        "    model_id=PEFT_MODEL,\n",
        "    #model_id=PEFT_COMPLETE_MODEL,\n",
        "    is_trainable=False,\n",
        "    cache_dir=HF_CACHE_DIR,\n",
        "    #torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60RnTwP7VYqD"
      },
      "outputs": [],
      "source": [
        "peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vafOl-5FVbSU"
      },
      "outputs": [],
      "source": [
        "generation_args = {\n",
        "    'pad_token_id': tokenizer.pad_token_id,\n",
        "    'eos_token_id': tokenizer.eos_token_id,\n",
        "    'bos_token_id': tokenizer.bos_token_id,\n",
        "    'do_sample': True,\n",
        "    'num_beams': 1,\n",
        "    'temperature': 0.25,\n",
        "}\n",
        "\n",
        "pipe = pipeline(task='conversational', model=peft_model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GIrWlCjVj5L"
      },
      "outputs": [],
      "source": [
        "pergunta = '''\\\n",
        "Identifique e extraia as informações de nomes de pessoas jurídicas mencionadas no texto, incluindo nomes de órgãos publicos, \\\n",
        "empresas, partidos políticos, associações, sindicatos ou quaisquer outras entidades. Não extraia nomes de pessoas físicas.\\\n",
        "O resultado deve ser apresentado em forma de lista de pessas jurídicas conforme o exemplo abaixo:\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Petrobrás é uma empresa. José da Silva é uma pessoa física.\n",
        "\n",
        "Pessoas Jurídicas: Petrobrás|Empresa\n",
        "\n",
        "Faça o mesmo para o seguinte texto:\n",
        "\n",
        "No fervilhante cenário político e econômico do Brasil, onde as decisões do Supremo Tribunal Federal e as políticas do Ministério da Economia \\\n",
        "têm repercussões que ecoam por todo o país, é comum observar figuras proeminentes como Maria Silva, CEO da renomada empresa de tecnologia \\\n",
        "Innovatech, buscando navegar entre os interesses corporativos e as demandas sociais. Enquanto isso, José Santos, um respeitado advogado, \\\n",
        "se destaca por sua atuação na defesa dos direitos individuais em meio às controvérsias jurídicas que frequentemente alcançam os tribunais \\\n",
        "superiores. Nas salas de reuniões, executivos como Ana Oliveira e Carlos Almeida da Vale Energia negociam estratégias de expansão \\\n",
        "e investimentos diante das flutuações do mercado global, enquanto o Ministério da Saúde, liderado pelo Ministro Pedro Lima, enfrenta desafios \\\n",
        "sem precedentes na gestão da saúde pública, especialmente em tempos de crise como a pandemia de COVID-19.\n",
        "'''\n",
        "\n",
        "output = pipe([\n",
        "    {'role': 'user', 'content': pergunta},\n",
        "], **generation_args)\n",
        "\n",
        "print(output.messages[-1]['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferência no modelo gerado com 2048 tokens"
      ],
      "metadata": {
        "id": "xqA-1xhoubVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Variável que define o diretório onde se encontra o modelo já tunado e que será carregado para a realização de testes de inferência. Estou deixando apontando para o modelo que já realizei o texto\n",
        "#com max_seq_lengthe fixado em 2048.\n",
        "\n",
        "#Aqui define apenas o diretório onde o modelo para teste de inferência será carregado, 256 ou 2048\n",
        "SENTENCE_MAX_LENGTH = 2048\n",
        "\n",
        "PEFT_MODEL = f'/content/drive/MyDrive/data/model/mistral2govbr_ner_' + str(SENTENCE_MAX_LENGTH) + '/'"
      ],
      "metadata": {
        "id": "k1KZHciMucK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Teste de inferência no modelo tunado\n",
        "print('Carregando o modelo de ' + PEFT_MODEL)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "     pretrained_model_name_or_path='fernandosola/bluearara-7B-instruct',\n",
        "     # load_in_4bit=True,\n",
        "     # quantization_config=BitsAndBytesConfig(\n",
        "     #     load_in_4bit=True,\n",
        "     #     bnb_4bit_compute_dtype=torch.float16,\n",
        "     #     bnb_4bit_use_double_quant=True,\n",
        "     #     bnb_4bit_quant_type=\"nf4\",\n",
        "     # ),\n",
        "     # torch_dtype=torch.bfloat16,\n",
        "     cache_dir=HF_CACHE_DIR,\n",
        "     force_download=False,\n",
        "     resume_download=False,\n",
        "     token='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'\n",
        " )\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "     PEFT_MODEL,\n",
        "     use_fast=False,\n",
        "     cache_dir=HF_CACHE_DIR,\n",
        "     token='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'\n",
        " )\n",
        "\n",
        "\n",
        "peft_model = PeftModelForCausalLM.from_pretrained(\n",
        "    model=model,\n",
        "    model_id=PEFT_MODEL,\n",
        "    #model_id=PEFT_COMPLETE_MODEL,\n",
        "    is_trainable=False,\n",
        "    cache_dir=HF_CACHE_DIR,\n",
        "    #torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "g3-gPKodupZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "id": "XO-2oSyGuxNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    'pad_token_id': tokenizer.pad_token_id,\n",
        "    'eos_token_id': tokenizer.eos_token_id,\n",
        "    'bos_token_id': tokenizer.bos_token_id,\n",
        "    'do_sample': True,\n",
        "    'num_beams': 1,\n",
        "    'temperature': 0.25,\n",
        "}\n",
        "\n",
        "pipe = pipeline(task='conversational', model=peft_model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "H13ZIQNXuxo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = '''\\\n",
        "Identifique e extraia as informações de nomes de pessoas jurídicas mencionadas no texto, incluindo nomes de órgãos publicos, \\\n",
        "empresas, partidos políticos, associações, sindicatos ou quaisquer outras entidades. Não extraia nomes de pessoas físicas.\\\n",
        "O resultado deve ser apresentado em forma de lista de pessas jurídicas conforme o exemplo abaixo:\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Petrobrás é uma empresa. José da Silva é uma pessoa física.\n",
        "\n",
        "Pessoas Jurídicas: Petrobrás|Empresa\n",
        "\n",
        "Faça o mesmo para o seguinte texto:\n",
        "\n",
        "No fervilhante cenário político e econômico do Brasil, onde as decisões do Supremo Tribunal Federal e as políticas do Ministério da Economia \\\n",
        "têm repercussões que ecoam por todo o país, é comum observar figuras proeminentes como Maria Silva, CEO da renomada empresa de tecnologia \\\n",
        "Innovatech, buscando navegar entre os interesses corporativos e as demandas sociais. Enquanto isso, José Santos, um respeitado advogado, \\\n",
        "se destaca por sua atuação na defesa dos direitos individuais em meio às controvérsias jurídicas que frequentemente alcançam os tribunais \\\n",
        "superiores. Nas salas de reuniões, executivos como Ana Oliveira e Carlos Almeida da Vale Energia negociam estratégias de expansão \\\n",
        "e investimentos diante das flutuações do mercado global, enquanto o Ministério da Saúde, liderado pelo Ministro Pedro Lima, enfrenta desafios \\\n",
        "sem precedentes na gestão da saúde pública, especialmente em tempos de crise como a pandemia de COVID-19.\n",
        "'''\n",
        "\n",
        "output = pipe([\n",
        "    {'role': 'user', 'content': pergunta},\n",
        "], **generation_args)\n",
        "\n",
        "print(output.messages[-1]['content'])"
      ],
      "metadata": {
        "id": "ieH7lfMQux9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}